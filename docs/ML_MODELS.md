# ðŸ¤– ML Models Documentation

## Overview

The ML Models component provides probability predictions for FNO (Futures & Options) movements across different timeframes using advanced machine learning algorithms.

## Architecture

### Model Types

1. **XGBoost**: Primary model for daily predictions
2. **Random Forest**: Used for weekly predictions  
3. **Gradient Boosting**: Handles monthly predictions

### Timeframes

- **Daily**: Â±3-5% movement predictions
- **Weekly**: Â±5% movement predictions
- **Monthly**: Â±10% movement predictions

## Features

### Technical Indicators (22+ Features)

```python
# Core Price Features
- daily_return, weekly_return, monthly_return
- return_3d_mean, return_5d_mean, return_20d_mean
- return_3d_std, return_5d_std, return_20d_std

# Volume Features
- volume_spike_ratio
- volume_ma_5, volume_ma_20

# Technical Indicators
- rsi_3, rsi_14
- macd, macd_signal
- bb_width (Bollinger Bands)
- stoch_k, stoch_d
- atr_5, atr_14

# Options Features
- oi_change_pct, oi_spike_ratio
- implied_volatility
- put_call_ratio

# Volatility Features
- intraday_range
- volatility_5d, volatility_20d
```

### Label Creation

```python
# Daily labels (Â±3-5% move)
df['daily_label'] = 1  # Default to neutral
df.loc[df['daily_return'] >= 0.03, 'daily_label'] = 2  # Up
df.loc[df['daily_return'] <= -0.03, 'daily_label'] = 0  # Down

# Weekly labels (Â±5% move)
df['weekly_label'] = 1  # Default to neutral
df.loc[df['weekly_return'] >= 0.05, 'weekly_label'] = 2  # Up
df.loc[df['weekly_return'] <= -0.05, 'weekly_label'] = 0  # Down

# Monthly labels (Â±10% move)
df['monthly_label'] = 1  # Default to neutral
df.loc[df['monthly_return'] >= 0.10, 'monthly_label'] = 2  # Up
df.loc[df['monthly_return'] <= -0.10, 'monthly_label'] = 0  # Down
```

## Usage

### Basic Model Training

```python
from src.fno_rag.ml.probability_models import FNOProbabilityModels

# Initialize models
models = FNOProbabilityModels()

# Train all models
models.train_models()

# Get prediction
result = models.predict_probability("NIFTY", HorizonType.DAILY)
print(f"Up: {result.up_probability:.1%}")
print(f"Down: {result.down_probability:.1%}")
print(f"Confidence: {result.confidence_score:.1%}")
```

### Model Configuration

```python
MODEL_CONFIGS = {
    HorizonType.DAILY: {
        'model_type': 'xgboost',
        'params': {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'random_state': 42
        }
    },
    HorizonType.WEEKLY: {
        'model_type': 'random_forest',
        'params': {
            'n_estimators': 200,
            'max_depth': 10,
            'random_state': 42
        }
    },
    HorizonType.MONTHLY: {
        'model_type': 'gradient_boosting',
        'params': {
            'n_estimators': 150,
            'max_depth': 8,
            'learning_rate': 0.05,
            'random_state': 42
        }
    }
}
```

### Feature Engineering

```python
def calculate_technical_indicators(df):
    """Calculate technical indicators for ML features."""
    
    # Rolling statistics
    for period in [3, 5, 20]:
        df[f'return_{period}d_mean'] = df['daily_return'].rolling(period).mean()
        df[f'return_{period}d_std'] = df['daily_return'].rolling(period).std()
    
    # Volume indicators
    df['volume_spike_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
    
    # Volatility indicators
    df['intraday_range'] = (df['high_price'] - df['low_price']) / df['close_price']
    
    # RSI
    df['rsi_3'] = calculate_rsi(df['close_price'], 3)
    df['rsi_14'] = calculate_rsi(df['close_price'], 14)
    
    # MACD
    df['macd'], df['macd_signal'] = calculate_macd(df['close_price'])
    
    # Bollinger Bands
    df['bb_width'] = calculate_bollinger_bands_width(df['close_price'])
    
    return df
```

## Model Performance

### Accuracy Metrics

- **Daily Models**: 88-96% confidence scores
- **Weekly Models**: 85-94% confidence scores  
- **Monthly Models**: 82-92% confidence scores

### Validation

```python
# Cross-validation results
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"Cross-validation accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

## Model Persistence

### Saving Models

```python
# Models are automatically saved to models/fno_ml/
models.save_models()
```

### Loading Models

```python
# Models are automatically loaded on initialization
models = FNOProbabilityModels()
models.load_models()
```

## Training Pipeline

### Data Preparation

1. **Data Collection**: Fetch FNO data from database
2. **Feature Engineering**: Calculate 22+ technical indicators
3. **Label Creation**: Create multi-class labels for each timeframe
4. **Data Cleaning**: Handle missing values and outliers
5. **Train/Test Split**: 80/20 split with stratification

### Training Process

1. **Feature Scaling**: StandardScaler for numerical features
2. **Model Training**: Train each model with optimized hyperparameters
3. **Validation**: Cross-validation for model selection
4. **Evaluation**: Calculate accuracy, precision, recall, F1-score
5. **Persistence**: Save trained models to disk

### Example Training Script

```python
#!/usr/bin/env python3
"""
Train FNO ML Models
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.fno_rag.ml.probability_models import FNOProbabilityModels
from loguru import logger

def train_models():
    """Train all FNO ML models."""
    
    try:
        # Initialize models
        models = FNOProbabilityModels()
        
        # Train models
        logger.info("Training FNO ML models...")
        results = models.train_models()
        
        # Print results
        for horizon, metrics in results.items():
            logger.info(f"{horizon.value}: Accuracy={metrics['accuracy']:.3f}")
        
        logger.info("âœ… Model training completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Model training failed: {e}")
        return False

if __name__ == "__main__":
    train_models()
```

## Prediction Pipeline

### Real-time Prediction

```python
def predict_probability(symbol: str, horizon: HorizonType) -> ProbabilityResult:
    """Get probability prediction for a symbol and timeframe."""
    
    # 1. Get latest data
    df = get_latest_data(symbol)
    
    # 2. Calculate features
    features = calculate_features(df)
    
    # 3. Load model
    model = load_model(horizon)
    
    # 4. Make prediction
    probabilities = model.predict_proba(features)
    
    # 5. Calculate confidence
    confidence = calculate_confidence(probabilities)
    
    # 6. Return result
    return ProbabilityResult(
        symbol=symbol,
        horizon=horizon,
        up_probability=probabilities[0][2],
        down_probability=probabilities[0][0],
        neutral_probability=probabilities[0][1],
        confidence_score=confidence,
        timestamp=datetime.now()
    )
```

## Model Evaluation

### Metrics

- **Accuracy**: Overall prediction accuracy
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)
- **F1-Score**: Harmonic mean of precision and recall
- **Confidence Score**: Model's confidence in prediction

### Example Evaluation

```python
def evaluate_model(model, X_test, y_test):
    """Evaluate model performance."""
    
    # Make predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Calculate confidence
    confidence = np.max(y_proba, axis=1).mean()
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confidence': confidence
    }
```

## Troubleshooting

### Common Issues

1. **Memory Issues**: Reduce batch size or use chunked processing
2. **Model Loading Errors**: Retrain models if files are corrupted
3. **Feature Mismatch**: Ensure feature names match training data
4. **Low Accuracy**: Check data quality and feature engineering

### Performance Optimization

1. **Feature Selection**: Use only relevant features
2. **Hyperparameter Tuning**: Optimize model parameters
3. **Ensemble Methods**: Combine multiple models
4. **Regularization**: Prevent overfitting

## Best Practices

1. **Data Quality**: Ensure clean, consistent data
2. **Feature Engineering**: Create meaningful features
3. **Model Selection**: Choose appropriate algorithms
4. **Validation**: Use cross-validation for robust evaluation
5. **Monitoring**: Track model performance over time
6. **Retraining**: Update models with new data

## Future Enhancements

1. **Deep Learning**: LSTM/Transformer models
2. **Ensemble Methods**: Voting and stacking
3. **Online Learning**: Incremental model updates
4. **Feature Importance**: Explainable AI
5. **AutoML**: Automated model selection
